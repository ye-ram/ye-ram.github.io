---
layout: post
title:  "Chap5. 트리 알고리즘"
date:   2024-08-21
categories: Machine Learning
---

## Chapter 5-1: 결정 트리

결정 트리는 데이터 전처리를 할 필요가 없다는 엄청난 장점을 가진다. 결정 과정을 이해하기 쉽기 때문에 제 3자에게 설명하기도 쉽다. 

![image](https://github.com/user-attachments/assets/ee67f7f9-cf95-47af-ae21-3ec54d5c8224)

1. **지니 불순도 Gini Impurity**

   **노드에서 데이터를 분할할 기준**으로 DecisionTreeClassifier 클래스의 criterion 매개변수의 기본값이다.

   ![image](https://github.com/user-attachments/assets/ad8e3842-6113-449e-adbe-9f74cf763413)

   결정 트리 모델은 **부모 노드와 자식 노드의 불순도 차이가 가장 크게 되도록** 트리를 분할시킨다. 즉, **informaton gain이 최대가 되도록** 데이터를 분할한다. 

2. **가지치기**

   결정 트리의 점수가 과대적합되어 보인다면 트리가 너무 깊지는 않은지 확인해 보아야 한다. max_depth를 통해 트리의 깊이를 지정할 수 있다.

   ```python
   dt = DecisionTreeClassifier(random_state=42, max_depth=3)
  ```

   특성 중요도를 통해 ‘sugar’ 당도가 가장 중요한 특성임을 알 수 있다.
  ```python
  print(dt.feature_importances_)
  [0.12345626 0.86862934 0.0079144 ]
  ```


## Chapter 5-2: 교차 검증과 그리드 서치

* 검증 세트

  훈련 세트 : 검증 세트 : 테스트 세트 = 6 : 2 : 2

  **훈련 세트**로 모델을 훈련하고 **검증 세트**로 모델을 평가한다. 테스트 해보고 싶은 매개변수를 모두 평가해본 뒤 가장 좋은 성능을 보인 모델을 **테스트 세트**로 최종 평가한다.

  ```python
    # 훈련 세트, 검증 세트, 테스트 세트로 나누는 방법
   from sklearn.model_selection import train_test_split
   train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state = 42)
  sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state = 42)
  ```

* k-폴드 교차 검증

  기존의 훈련 세트를 훈련 세트와 검증 세트로 나누어서 훈련에 필요한 데이터의 수가 부족해지는 문제가 발생하기도 한다.

  적은 데이터 수로도 안정적인 검증 점수를 얻기 위해서 교차 검증을 사용한다.

  ![image](https://github.com/user-attachments/assets/7af6a14a-6235-489f-84fa-51dd7801c156)

  cross_validate( ) 함수는 기본적으로 5개로 훈련 세트를 나누어서 교차 검증을 수행한다.

  ```python
  from sklearn.model_selection import cross_validate
  scores = cross_validate(dt, train_input, train_target)
  print(scores)
  
  {'fit_time': array([0.0125494 , 0.00740671, 0.00760531, 0.0075438 , 0.00715327]), 
  'score_time': array([0.00130272, 0.00112629, 0.00111675, 0.00112343, 0.0011127 ]), 
  'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
  
  # fit_time은 훈련하는 시간, score_time은 검증하는 시간을 의미한다. 
  ```
  위 코드에서 test_score은 5개의 검증 점수이기 때문에 5개의 점수를 평균해서 최종 검증 점수를 출력한다.
  
  ```python
  import numpy as np
  print(np.mean(scores['test_score']))
  ```

  cross_validate( ) 함수는 훈련 세트를 섞어서 폴드를 나누지 않기 때문에 훈련 세트를 섞기 위해서는 분할기(splitter)를 정해야 한다. 기본적으론 K-fold 교차 검증을 사용한다.
  
  ```python
  from sklearn.model_selection import cross_validate
  from sklearn.model_selection import StratifiedKFold
  splitter = StratifiedKFold(n_splits = 10, shuffle=True, random_state=42)
  scores = cross_validate(dt, train_input, train_target, cv=splitter)
  print(np.mean(scores['test_score']))
  ```
  회귀 모델: K-Fold  / 분류 모델: StratifiedKFold

* 하이퍼 파라미터 튜닝

  모델에게 가장 적합한 하이퍼 파라미터 찾기

  *하이퍼 파라미터: 모델을 생성할 때 사용자가 직접 설정하는 변수 *

  1. **그리드 서치 Grid Search**

     내가 설정한 범위 안에서의 최적 파라미터를 찾는다. 모든 경우에서의 최적 파라미터라고 볼 수 없다! 더 나은 결과를 내는 파라미터가 존재할 수도 있고 안 할 수도 있다는 말!

     GridSearhCV 클래스는 하이퍼 파라미터 탐색과 교차 검증을 한 번에 수행한다.

     ```python
     from sklearn.model_selection import GridSearchCV
     params = {'min_impurity_decrease':np.arange(0.0001, 0.001, 0.0001),
               'max_depth' : range(5, 20, 1),
               'min_samples_split': range(2, 100, 10)
               }
     # np.arange( )는 정수, 소수 둘 다 사용 가능하고 range( )는 정수에만 사용 가능하다.
    
     gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs=-1)
     gs.fit(train_input, train_target)
     print(gs.best_params_) # 최상의 파라미터 출력
     print(gs.cv_results_['mean_test_score']) # 각 매개변수의 검증점수 평균
    
     np.argmax( )는 배열에서 가장 큰 값의 인덱스를 반환하고 np.max( )는 배열에서 가장 큰 값을 반환한다. 
     ```

  2. **랜덤 서치 Random Search**

     그리드 서치보다 훨씬 교차 검증 수를 줄이면서 넓은 영역을 효과적으로 탐색할 수 있다. 그리드 서치에서는 하이퍼 파라미터 값의 목록을 전달했지만 랜덤 서치에서는 하이퍼 파라미터가 될 수 있는 값의 확률 분포를 전달하기 때문이다.

     ```python
     from scipy.stats import uniform, randint
     params = {'min_impurity_decrease' : uniform(0.0001, 0.001),
               'max_depth' : randint(20, 50),
               'min_samples_split': randint(2, 25),
               'min_samples_leaf' : randint(1, 25)
               }
     from sklearn.model_selection import RandomizedSearchCV
     gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42, splitter='random'), params, n_iter=100, n_jobs=-1, random_state=42)
     gs.fit(train_input, train_target)
     ```

## Chapter 5-3: 트리의 앙상블

정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 앙상블 학습. 앙상블 학습은 대부분 결정 트리를 기반으로 만들어져 있다. 

> 정형 데이터-어떤 구조로 되어있음(CSV, DB, 엑셀에 저장) → 앙상블 학습
> 비정형 데이터-텍스트 데이터, 사진, 음악 → 신경망 알고리즘

* 랜덤 포레스트

  - 부트스트랩(bootstrap) 사용: 데이터를 중복 허용해서 랜덤하게 선택 (부스스트랩 ~ bagging  ?)
 
  - 노드 분할도 랜덤하게 선택한 뒤 가장 최선의 분할을 찾음
 
  결정 트리에서 만든 특성 중요도 [ 0.1235 0.8686 0.0079] 

  랜덤 포레스트의 특성 중요도 [ 0.2317 0.5004 0.2679]

  → 랜덤 포레스트에서 각 특성의 중요도가 좀 더 고른 결과를 보인다. 랜덤하게 결정 트리를 훈련하기 때문에 더 많은 특성이 훈련에 기여할 기회를 얻어 과대적합을 줄일 수 있다.

  OOB 점수: 훈련 세트 중에서 부트스트랩 샘플에 포함되지 않는 데이터를 OOB 샘플 이라고 한다. OOB 샘플로 모델을 검증하는 방식이 OOB 점수이다. 기존의 훈련 세트를 훈련 세트과 검증 세트로 나누지 않고도 검증할 수 있기 때문에 훈련에 사용되는 데이터의 수가 많아지는 장점이 있다.

  ```python
  rf = RandomForestClassifier(oob_score = True, n_jobs=-1, random_state=42)
  rf.fit(train_input, train_target)
  print(rf.oob_score_)
  ```

* 엑스트라 트리

  RF와 다르게 부트스트랩을 사용하지 않고 전체 훈련 세트를 사용한다. 노드 분할도 랜덤하게 진행하기 때문에 최선의 분할이 아닐 수도 있다.

  <장점> 랜덤하게 노드를 분할해서 계산 속도가 빠름

  <단점> 노드를 무작위로 분할하기 때문에 성능이 낮아진다.
  
* 그래디언트 부스팅 Gradient Boosting

  깊이가 얕은 트리를 사용하는 방식이다. 디폴트는 깊이 = 3인 결정 트리 100개를 사용한다. 기본적으로는 훈련 세트를 모두 사용한다.

  이름에서 알 수 있듯이 Gradient 방식을 사용하는데, 결정 트리를 하나씩 추가하면서 손실 함수가 가장 적은 지점으로 이동한다.

  ```python
  from sklearn.ensemble import GradientBoostingClassifier
  gb = GradientBoostingClassifier(random_state=42)
  scores = cross_validate(gb, train_input, train_target, n_jobs=-1, return_train_score=True)
  print(np.mean(scores['train_score']), np.mean(scores['test_score']))
  0.8881086892152563 0.8720430147331015
  
  print(gb.feature_importances_)
  [0.20520701 0.61546058 0.17933242]
  ```

  일반적으로 그래디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있지만 Gradient 방식을 사용해 하나하나씩 트리를 추가하기 때문에 훈련 속도가 느린 단점이 있다.
  
* 히스토그램 기반 그래디언트 부스팅 Histogram-based Gradient Boosting

  그래디언트 부스팅의 속도를 개선한 모델이다.

  입력 특성을 256개 구간으로 나누기 때문에 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.

  누락된 값이 있어도 상관없다. 256개 구간 중 하나를 떼어 놓고 누락된 값을 위해 사용하기 때문!

  머신러닝 알고리즘 중 가장 인기가 높다고 한다..!

  > 다른 히스토그램 기반 그래디언트 부스팅 라이브러리: XGBoost, LightGBM

| 랜덤 포레스트 | 엑스트라 트리 |그래디언트 부스팅|히스토그램 기반 그래디언트 부스팅|
|:---:|:---:|:---:|:---:|
|전체에서 중복 추출|전체 훈련 데이터 사용|전체 훈련 데이터 사용(subsample 매개변수로 조정 가능)|-|
|가장 좋은 분할 선택|노드 분할은 랜덤하게|훈련 속도가 느림|그래디언트 부스팅의 느린 속도 개선한 모델|





