---
layout: post
title:  "Chap6. 비지도 학습"
date:   2024-08-28
categories: Machine Learning
---

## Chapter 6-1: 군집 알고리즘

1. **비지도 학습**

   타겟을 제공하지 않고 인공지능이 입력 세트에서 패턴과 상관관계를 찾는 알고리즘

   군집: 비슷한 샘플끼리 그룹으로 모은다. 비지도 학습 작업 중 하나. 

2. **픽셀값 분석하기**

   2차원 배열로 이루어진 이미지를 1차원으로 바꾸고 aixs=1을 기준으로 각 사진의 픽셀값을 평균내어 그래프로 나타내면 아래와 같다. apple과 pineapple의 구분이 어려워 보이기 때문에 다른 방법이 필요하다.

   ![image](https://github.com/user-attachments/assets/8b9b3b04-dfcb-4463-9fef-7807469fd7bd)

   샘플의 평균값을 비교하기 보다 픽셀별 평균값을 비교해보면 아래와 같다. axis = 0을 기준으로 평균값을 계산하는데 각 그래프마다 값이 높은 구간이 다르다는 것을 알 수 있다.

   ![image](https://github.com/user-attachments/assets/351cd12e-00c2-455a-b50e-a813e917cfa4)

   ![image](https://github.com/user-attachments/assets/8c0fd99a-e1fa-49ab-b566-e64de90bc28f)

   *차원에 따라 축이 바뀌는 거 주의!

   아래 코드는 사과 사진의 평균값인 apple.mean과 가장 가까운 사진 100개를 출력하는 코드이다. 

    ```python
    abs_diff = np.abs(fruits - apple_mean) #(300, 100, 100) 크기의 배열
    abs_mean = np.mean(abs_diff, axis=(1, 2))
    
    apple_index = np.argsort(abs_mean)[:100] # np.argsort()는 작은 것에서 큰 순서대로 나열한 abs_mean배열의 인덱스를 반환한다. 
    fig, axs = plt.subplots(10, 10, figsize=(10, 10))
    for i in range(10):
      for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap = 'gray_r')
        axs[i, j].axis('off')
    plt.show()
    ```

   이 단원에서는 사과, 바나나, 파인애플과 같이 타깃에 대한 정보가 존재한 상황이었기 때문에 각 타겟의 픽셀 평균값을 사용해 분류할 수 있었다. 타깃에 대한 정보가 없다면 어떻게 타깃의 평균값을 알 수 있을까?

## Chapter 6-2: 군집 알고리즘

1. **k-means 알고리즘**

   * 무작위로 k개의 클러스터 중심을 정한다.
  
   * 각 샘플이 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 정한다.
  
   * 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
  
   * 클러스터의 중심이 변하지 않을 때까지 반복한다.

    ```python
   from sklearn.cluster import KMeans
   km = KMeans(n_clusters=3, random_state=42)
   km.fit(fruits_2d)
  
   print(km.labels_) # 클래스 객체의 속성이 출력된다.
   print(np.unique(km.labels_, return_counts=True))
   ```

  km.cluster_centers_: KMeans 클래스가 최종적으로 찾은 클러스터 중심이 저장되어 있음

  km.transform: 훈련 데이터 샘플이 각 클러스터 중심과의 거리가 얼마인지 출력

  km.predict: 가장 가까운 클러스터 중심을 예측 클래스로 출력한다.

2. **최적의 k 찾기**

   적절한 클러스터의 개수를 찾기 위해서는 대표적으로 **elbow 방법**을 사용한다. 클러스터 중심과 클러스터에 속한 샘플 사이의 거리 제곱 합을 이너셔(inertia)라고 부른다. **이너셔가 줄어드는 속도가 꺾이는 지점을 최적 클러스터 개수로 결정한다.**

   엘보우 지점은 클러스터 수가 증가하면서 응집도(이너셔)가 급격히 개선되다가 그 이후에는 개선 폭이 줄어드는 지점이다. 클러스터 수를 늘려도 응집도가 큰 폭으로 감소하지 않기 때문에, 추가적인 클러스터 수가 실제로 데이터의 구조를 더 잘 설명하지 못하게 된다.

   * km.inertia_: 이너셔를 계산하는 클래스

   ![image](https://github.com/user-attachments/assets/d4c28bb9-1777-4bf7-b111-4c9f24dd3e8c)


## Chapter 6-3: 주성분 분석

1. **PCA 클래스**

   주성분 분석은 데이터에 있는 분산이 큰 방향을 찾는 과정이다. 원본 데이터를 주성분에 투영하면 새로운 특성을 만들 수 있다. 주성분은 원본 특성의 개수만큼 찾을 수 있고 주성분으로 바꾼 데이터는 차원이 줄어들게 된다.

   ```python
   from sklearn.decomposition import PCA
   pca = PCA(n_components=50) # n_components에 주성분의 개수또는 설명된 분산의 비율을 지정한다.
   pca.fit(fruits_2d)
   fruits_pca = pca.transform(fruits_2d)
   ```

   50개의 주성분은 분산이 가장 큰 순서대로 나열된다.

   fruits_2d는 (300, 10000) 크기의 데이터로 10000개의 특성을 가진 300개의 이미지이다. pca 차원 축소를 사용한 fruits_pca는 (300, 50) 크기로 50개의 특성을 가진 300개의 데이터이다.
   
2. **원본 데이터 재구성**

   inverse_transform() 메서드를 통해 줄인 특성을 원래대로 복원할 수 있다. 주성분 50개가 데이터의 특성을 잘 보존하도록 선정된 것이기 때문에 50개의 주성분을 가지고 원래 특성으로 복원할 수 있는 것이다.

   ```python
   fruits_inverse = pca.inverse_transform(fruits_pca)
   print(fruits_inverse.shape)
   (300, 10000)
   ```

3. **설명된 분산**

   설명된 분산은 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값이다. 첫 번째 주성분의 설명된 분산이 가장 크다. pca.explained_variance_ratio_를 통해 확인할 수 있다.

   모든 주성분의 설명된 분산을 더하면 50개의 주성분으로 표현하고 있는 총 분산 비율을 얻을 수 있다. 아래 코드에서는 92%인데 원본 데이터의 총 분산 중 92%를 설명하고 있다는 뜻이다.

   ```python
   print(np.sum(pca.explained_variance_ratio_))
   0.9214797037087654
   ```


