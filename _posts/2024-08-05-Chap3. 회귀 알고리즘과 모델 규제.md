---
layout: post
title:  "Chap3. 회귀 알고리즘과 모델 규제"
date:   2024-08-05
categories: Machine Learning
---

## Chapter 3-1. k-최근접 이웃 회귀

1. **k-최근접 이웃 회귀**

   예측하려는 샘플에 가장 가까운 k개의 샘플을 고른다. k개의 샘플의 평균 값 = 샘플의 예측값

   * reshape(): 사이킷런에 사용할 train data는 2차원 배열이어야 한다. reshape()은 배열의 크기를 바꿔준다.
   
     ```python
     train_input = train_input.shape(-1, 1)
     test_input = test_input.shape(-1, 1)
     ```

2. **훈련시킨 모델의 평가**

   회귀의 경우 밑의 두 가지 방식으로 모델의 평가를 한다.

   * 결정계수(R^2): 1에 가까울수록 좋다.

     ```python
     knr.score(test.input, test.target)
     ```

   * MAE mean_absolute_error(): |실제값과 예측값의 차| 의 평균

     ```python
     from sklearn.metrics import mean_absolute_error

     test_prediction = knr.predict(test.input)
     mae = mean_absolute_error(test_target, test_prediction)
     print(mae) # 19.1571 
      ```

     MAE가 19라면 예측이 평균적으로 19 정도 실제값과 다르다는 것을 나타낸다.

     ![image](https://github.com/user-attachments/assets/329fa292-fc8f-4bd1-92c2-f0c17f075fe8)

   
3. **과대적합 vs 과소적합**

   * 과대적합(over-fitting): 모델이 train data에만 잘 맞는 경우에 발생한다.

     훈련 세트에서는 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 안 좋은 경우.

     해결방법: 모델을 덜 복잡하게(k-최근접 이웃은 k값 늘리기)
     
   * 과소적합(underfitting): 모델이 너무 단순해서 훈련 세트에 적절히 훈련되지 않거나 데이터의 크기가 작은 경우에 발생한다.(데이터의 크기가 작으면 테스트 세트가 훈련 세트의 특징을 따르지 못 하는 경우도 있다.)

      훈련 세트는 점수가 안 좋지만 테스트 세트의 점수가 좋은 경우. 둘 다 낮은 경우.

      해결방법: 모델을 더 복잡하게(k-최근접 이웃은 k값 줄이기)





## Chapter 3-2. 선형 회귀

k-최근접 이웃 회귀 단점: 훈련 세트 범위 밖의 샘플을 예측할 수 없다. 새로운 데이터가 기존 데이터에서 아무리 멀리 떨어져 있다 하더라도 무조건 가장 가까운 샘플의 타깃을 평균해 예측하기 때문

밑의 그래프에서 삼각형은 그래프의 오른쪽 위에 위치해야 한다고 예상할 수 있지만 실제 예측값은 1033g이다. 가장 근처에 있는 세 데이터를 참고해 예측된 값이기 때문.  

![image](https://github.com/user-attachments/assets/3b9855be-208a-4e33-95a7-e3cbf54d904c)

1. **선형 회귀**

   y = ax + b의 **직선 그래프**를 학습하는 알고리즘

   ![image](https://github.com/user-attachments/assets/df2002b8-f0cc-4b17-9394-2452abc4dd60)

   ```python
   from sklearn.linear_model import LinearRegression
   lr = LinearRegression()
   lr.fit(train_input, train_target)

   print(lr.coef_, lr.intercept_)  # 선형 방정식 y=ax+b 의 a와 b
    ```
   평가 방법: R^2 점수
   ```python
   print(lr.score(train_input, train_target))
    print(lr.score(test_input, test_target))
    ```

   
2. **다항 회귀**
   
   **다항식**을 사용한 선형 회귀

   ![image](https://github.com/user-attachments/assets/4cad525a-3e12-41ca-b3c0-d447a86dab57)

   x값이 있으면 이 값을 제곱해 x^2을 만들어 학습시킨다. 

   ```python
   from sklearn.linear_model import LinearRegression
   lr = LinearRegression()
   lr.fit(train_poly, train_poly)
   print(lr.predict([[50**2, 50]]))

   print(lr.coef_, lr.intercept_) # [1.01  -21.6] 116.05
															 # 선형 방정식 y=ax^2+bx+c 의 a와 b와 c
    ```
    평가 방법은 선형회귀와 마찬가지로 R^2 점수를 사용한다. 
    ```python
    print(lr.score(train_poly, train_target))
    print(lr.score(test_poly, test_target))
    ```




## Chapter 3-3. 특성 공학과 규제

선형 회귀는 특성이 많을수록 엄청난 효과를 낸다.

길이, 높이 데이터 뿐만 아니라 두께 데이터도 활용해 본다.

1. **선형 회귀(다항 회귀) vs 다중 회귀**

   선형 회귀는 변수가 하나이고 다중 회귀는 변수가 두 개 이상이다.

   ![image](https://github.com/user-attachments/assets/c5c79522-a66d-4868-a2c8-0b93fa5919f6)
   
2. **새로운 특성 조합 만들기**

   특성 공학: 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업

   ```python
   from sklearn.preprocessing import PolynomialFeatures
   poly = PolynomialFeatures()
	     # 매개변수들
	     # include_bias=False 절편을 위한 항 1이 제거되고 출력됨.
	     # degree 고차항의 최대 차수 결정(degree=5이면 5제곱까지 특성 만듦)
   poly.fit([[2, 3]])
   print(poly.transform([[2, 3]]))

   [[1. 2. 3. 4. 6. 9.]]    
   ```
  [[1. 2. 3. 4. 6. 9.]] PolynomialFeatures를 사용하면 새로운 특성이 나온다. 
   ```python
   poly.get_feature_names()
   # ['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2']
   # 각 특성이 어떻게 만들어졌는지 확인할 수 있다.
   ```

3. **계수 정규화**

   다중 회귀 모델을 돌렸는데 test_data 의 결정 계수가 음수가 나온다면 계수 정규화를 먼저 실시한다. 특성의 개수가 많아지면 모델이 train_data에 정확히 피팅되는 일이 발생한다. train_data에 과대적합되므로 test_data에서는 점수가 이상하게 나오는 것이다.
   
   ```python
   #특성에 곱해지는 계수 정규화
   from sklearn.preprocessing import StandardScaler
   ss = StandardScaler()
   ss.fit(train_poly)
   train_scaled = ss.transform(train_poly)
   test_scaled = ss.transform(test_poly)
   ```

4. **규제**

   규제는 머신러닝 모델이 훈련 세트를 과도하게 학습하는 것을 막는 방법이다.

   선형 회귀 모델에 규제를 추가한 모델을 릿지와 라쏘라 부른다.

   매개변수 alpha로 규제의 강도를 조절. (alpha 크면 규제도 크다. 계수 값을 줄이고 과소적합 하도록 한다.)

   * 릿지 회귀

     계수를 제곱한 값을 기준으로 규제를 적용한다. 계수를 작게 만들어 과대적합을 막는다. 성능이 좋아 일반적으로 더 선호된다.

     ```python
     import matplotlib.pyplot as plt
     train_score = []
     test_score = []
     alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]

     for i in alpha_list:
       ridge = Ridge(alpha = i)
       ridge.fit(train_scaled, train_target)
       train_score.append(ridge.score(train_scaled, train_target))
       test_score.append(ridge.score(test_scaled, test_target))
     ```
     ```python
     plt.plot(np.log10(alpha_list), train_score)
     plt.plot(np.log10(alpha_list), test_score)
     plt.xlabel('alpha')
     plt.ylabel('R^2')
     plt.show()
     ```

     ![image](https://github.com/user-attachments/assets/991cbd8f-9d13-40aa-b727-58bb04741b30)

     alpha 값은 위 방법대로 정할 수 있다. train data와 test data가 가장 가깝고 test data의 점수가 가장 높은 alpha=-1 이 적절한 값이다.

   * 라쏘 회귀

     계수의 절대값을 기준으로 규제를 적용한다. 계수 중 일부가 0이 되기 때문에 유용한 특성을 골라내는 용도로 사용 가능하다.

     ```python
     from sklearn.linear_model import Lasso
     lasso = Lasso(alpha=10)
     lasso.fit(train_scaled, train_target)
     print(lasso.score(train_scaled, train_target))
     print(lasso.score(test_scaled, test_target))

     # lasso.coef_ 라쏘 모델의 계수가 저장된 속성
     ```




   

