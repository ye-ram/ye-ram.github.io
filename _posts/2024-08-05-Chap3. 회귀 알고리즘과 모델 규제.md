---
layout: post
title:  "Chap3. 회귀 알고리즘과 모델 규제"
date:   2024-08-05
categories: Machine Learning
---

## Chapter 3-1. k-최근접 이웃 회귀

1. **k-최근접 이웃 회귀**

   예측하려는 샘플에 가장 가까운 k개의 샘플을 고른다. k개의 샘플의 평균 값 = 샘플의 예측값

   * reshape(): 사이킷런에 사용할 train data는 2차원 배열이어야 한다. reshape()은 배열의 크기를 바꿔준다.
   
     ```python
     train_input = train_input.shape(-1, 1)
     test_input = test_input.shape(-1, 1)
     ```

2. **훈련시킨 모델의 평가**

   회귀의 경우 밑의 두 가지 방식으로 모델의 평가를 한다.

   * 결정계수(R^2): 1에 가까울수록 좋다.

     ```python
     knr.score(test.input, test.target)
     ```

   * MAE mean_absolute_error(): |실제값과 예측값의 차| 의 평균

     ```python
     from sklearn.metrics import mean_absolute_error

     test_prediction = knr.predict(test.input)
     mae = mean_absolute_error(test_target, test_prediction)
     print(mae) # 19.1571 
      ```

     MAE가 19라면 예측이 평균적으로 19 정도 실제값과 다르다는 것을 나타낸다.

     ![image](https://github.com/user-attachments/assets/329fa292-fc8f-4bd1-92c2-f0c17f075fe8)

   
3. **과대적합 vs 과소적합**

   * 과대적합(over-fitting): 모델이 train data에만 잘 맞는 경우에 발생한다.

     훈련 세트에서는 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 안 좋은 경우.

     해결방법: 모델을 덜 복잡하게(k-최근접 이웃은 k값 늘리기)
     
   * 과소적합(underfitting): 모델이 너무 단순해서 훈련 세트에 적절히 훈련되지 않거나 데이터의 크기가 작은 경우에 발생한다.(데이터의 크기가 작으면 테스트 세트가 훈련 세트의 특징을 따르지 못 하는 경우도 있다.)

      훈련 세트는 점수가 안 좋지만 테스트 세트의 점수가 좋은 경우. 둘 다 낮은 경우.

      해결방법: 모델을 더 복잡하게(k-최근접 이웃은 k값 줄이기)


## Chapter 3-2. 선형 회귀

k-최근접 이웃 회귀 단점: 훈련 세트 범위 밖의 샘플을 예측할 수 없다. 새로운 데이터가 기존 데이터에서 아무리 멀리 떨어져 있다 하더라도 무조건 가장 가까운 샘플의 타깃을 평균해 예측하기 때문

밑의 그래프에서 삼각형은 그래프의 오른쪽 위에 위치해야 한다고 예상할 수 있지만 실제 예측값은 1033g이다. 가장 근처에 있는 세 데이터를 참고해 예측된 값이기 때문.  

![image](https://github.com/user-attachments/assets/3b9855be-208a-4e33-95a7-e3cbf54d904c)

1. **선형 회귀**

   y = ax + b의 **직선 그래프**를 학습하는 알고리즘

   ![image](https://github.com/user-attachments/assets/df2002b8-f0cc-4b17-9394-2452abc4dd60)

   ```python
   from sklearn.linear_model import LinearRegression
   lr = LinearRegression()
   lr.fit(train_input, train_target)

   print(lr.coef_, lr.intercept_)  # 선형 방정식 y=ax+b 의 a와 b
    ```
   평가 방법: R^2 점수
   ```python
   print(lr.score(train_input, train_target))
    print(lr.score(test_input, test_target))
    ```

   
2. **다항 회귀**
   **다항식**을 사용한 선형 회귀

   ![image](https://github.com/user-attachments/assets/4cad525a-3e12-41ca-b3c0-d447a86dab57)

   x값이 있으면 이 값을 제곱해 x^2을 만들어 학습시킨다. 

   ```python
   from sklearn.linear_model import LinearRegression
   lr = LinearRegression()
   lr.fit(train_poly, train_poly)
   print(lr.predict([[50**2, 50]]))

   print(lr.coef_, lr.intercept_) # [1.01  -21.6] 116.05
															 # 선형 방정식 y=ax^2+bx+c 의 a와 b와 c
    ```
    평가 방법은 선형회귀와 마찬가지로 R^2 점수를 사용한다. 
    ```python
    print(lr.score(train_poly, train_target))
    print(lr.score(test_poly, test_target))
    ```

    

