---
layout: post
title:  "Chap4. 다양한 분류 알고리즘"
date:   2024-08-05
categories: Machine Learning
---

## Chapter 4-1: 로지스틱 회귀

> 분류 모델은 예측뿐만 아니라 예측의 근거가 되는 확률을 출력할 수 있다. 

1. **k-최근접 이웃**

   확률을 출력할 수 있지만 이웃한 샘플의 클래스 비율을 출력하므로 항상 정해진 확률만을 출력하는 한계가 있다. 

2. **로지스틱 회귀**

   로지스틱 회귀: 선형 방정식을 사용한 분류 알고리즘

   로지스틱 회귀는 사실 회귀가 아니라 분류다!

   * 이진분류

     선형 방정식을 학습한 뒤 방정식의 값에 시그모이드  함수를 취해 0~1사이의 값으로 변환한다. 

     시그모이드 함수의 출력이 0.5보다 크면 양성 클래스, 0.5보다 작으면 음성 클래스로 판단한다.

     ![image](https://github.com/user-attachments/assets/6e276225-7bc2-4cae-8502-5402aca8cc69)

     ```python
     from sklearn.linear_model import LogisticRegression
     lr = LogisticRegression()
     lr.fit(train_bream_smelt, target_bream_smelt)

     print(lr.predict(train_bream_smelt[:5])) 
      ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']

     print(lr.predict_proba(train_bream_smelt[:5]))
      [[0.99759855 0.00240145]
       [0.02735183 0.97264817]
       [0.99486072 0.00513928]
       [0.98584202 0.01415798]
       [0.99767269 0.00232731]] # 첫번째 열은 Bream의 확률. 두번째 열은 Smelt의 확률

     print(lr.classes_) # 항상 알파벳 순으로!
      ['Bream' 'Smelt']
      
     print(lr.coef_, lr.intercept_) # 선형방정식의 계수와 상수항
      [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
      ```

     ![image](https://github.com/user-attachments/assets/89ce6b78-19de-45a9-bdc2-c7a78e44eecd)

     ```python
     decisions = lr.decision_function(train_bream_smelt[:5]) # lr.decision_fuction()은 z값 출력하는 함수
     print(decisions)
      [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
    
     from scipy.special import expit
     print(expit(decisions)) # expit()은 시그모이드 함수의 y값을 출력한다. 
      [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
     ```

   * 다중분류

     소프트맥스 함수를 사용해 z값을 0~1사이의 값으로 변환시켜 다중 분류한다.

     ```python
     lr = LogisticRegression(C=20, max_iter=1000) #로지스틱 회귀에서 C가 감소하면 규제는 증가
     lr.fit(train_scaled, train_target)
     print(lr.score(train_scaled, train_target))
     print(lr.score(test_scaled, test_target))
      0.9327731092436975
      0.925
    
     lr.predict(test_scaled[:5])
     array(['Perch', 'Smelt', 'Pike', 'Roach', 'Perch'], dtype=object)
    
     proba = lr.predict_proba(test_scaled[:5])
     print(np.round(proba, decimals=3))
      [[0.    0.014 0.841 0.    0.136 0.007 0.003]
       [0.    0.003 0.044 0.    0.007 0.946 0.   ]
       [0.    0.    0.034 0.935 0.015 0.016 0.   ]
       [0.011 0.034 0.306 0.007 0.567 0.    0.076]
       [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    
     print(lr.classes_)
      ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    
     print(lr.coef_.shape, lr.intercept_.shape)
      (7, 5) (7,)
     ```

     다중 분류를 할 때는 5개의 특성을 사용하므로 열은 5개이고, 7개의 예측값을 출력해야 하기 때문에 행은 7개이다. 다중 분류는 클래스마다 z값을 하나씩 계산한다. 

     총 7개의 선형방정식이 나오고 그 중 가장 값이 큰 값으로 예측한다.

     ```python
     decision = lr.decision_function(test_scaled[:5])
     print(np.round(decision, decimals=2))
      [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
       [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
       [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
       [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
       [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
    
     from scipy.special import softmax # 소프트맥스 함수 사용
     proba = softmax(decision, axis=1) # axis=1로 지정해야 각 행에 대해 소프트맥스를 취한다. 
     print(np.round(proba, decimals=3))
      [[0.    0.014 0.841 0.    0.136 0.007 0.003]
       [0.    0.003 0.044 0.    0.007 0.946 0.   ]
       [0.    0.    0.034 0.935 0.015 0.016 0.   ]
       [0.011 0.034 0.306 0.007 0.567 0.    0.076]
       [0.    0.    0.904 0.002 0.089 0.002 0.001]]

      ```

     소프트맥스 계산 방법

     ![image](https://github.com/user-attachments/assets/49de22fe-73e3-4069-95d1-378a62549b22)


## Chapter 4-2: 확률적 경사 하강법

점진적 학습: 학습 데이터가 계속 추가되는 상황에서 새로운 데이터에 대해서만 학습한다. 이전에 훈련한 모델을 버리지 않는 방법이다. 

1. **확률적 경사 하강법**

   대표적인 점진적 학습 알고리즘. 손실 함수라는 산을 정의하고 가장 가파른 경사를 따라 조금씩 내려오는 알고리즘이다. 

   훈련 세트에서 랜덤하게 하나의 샘플을 고른다. > 경사를 조금 내려온다. > 또 샘플을 고른다. > 내려온다. // 샘플을 다 썼는데도 산을 내려오지 못했으면 처음부터 다시 한다.

   ** 확률적 경사 하강법: 1개씩 샘플을 꺼내 경사를 내려감

   ** 미니배치 경사 하강법: 여러 개의 샘플을 사용해 경사 하강법 수행

   ** 배치 경사 하강법: 한 번 경사로를 내려가기 위해 전체 샘플을 사용

   > 신경망 알고리즘이 확률적 경사 하강법, 미니배치 경사 하강법 이용한다는거 기억!


2. **손실 함수**

   ![image](https://github.com/user-attachments/assets/746f7c40-8fa6-4cf9-834f-2e2d588c1e16)

   이진 분류 → 로지스틱 손실 함수 사용

   다중 분류 → 크로스 엔트로피 손실 함수 사용

   회귀 → 평균 제곱 오차(Mean Squared Error) 사용


3. **SGDClassifier**

   확률적 경사 하강법을 사용한 분류 모델

   매개변수 지정

     loss는 손실 함수의 종류를 지정한다. (log는 로지스틱 손실 함수, hinge는 서포트 벡터 머신)

     max_iter는 수행할 에포크 횟수를 지정한다.

   ```python
   from sklearn.linear_model import SGDClassifier
   sc = SGDClassifier(loss='log', max_iter=10, random_state=42) 
   sc.fit(train_scaled, train_target)
   print(sc.score(train_scaled, train_target))
   print(sc.score(test_scaled, test_target))
  
   sc.partial_fit(train_scaled, train_target) # partial_fit()을 한 번 호출할 때마다 1에포크 씩 이어서 훈련 가능
   print(sc.score(train_scaled, train_target))
   print(sc.score(test_scaled, test_target))
   ```

   에포크를 몇 번 실행했는지에 따라 모델의 과소적합, 과대적합이 될 수 있다. 에포크가 너무 적다면 과소적합, 너무 많다면 과대적합된 모델일 가능성이 높다.

   훈련을 반복 진행해서 과소적합, 과대적합이 되지 않는 에포크 횟수를 구한다. 아래 그래프에서는 epoch = 100이 적절한 반복 횟수이다.

   ![image](https://github.com/user-attachments/assets/1b337d1b-2b2d-49d4-9a98-a74522cc75a4)

   
   








